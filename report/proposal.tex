\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=2cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}


\usepackage[nottoc]{tocbibind}

% pronkinalexeyviktorovich@gmail.com


\title{Project report \\ Improving Explorability in Variational Inference with Annealed Variational Objectives}
\author{Mikhail Kurenkov, Timur Chikichev, Aleksei Pronkin}
\date{23 October 2020}

\begin{document}

\maketitle

Project Github repository:
\url{https://github.com/alexey-pronkin/annealed}

\subsection*{Introduction}

Variational Inference ~ reducing representational bias ~ amortized VI, Variational Autoencoders (VAE)
Expressive families of variational distributions >> losing the computational tractability
Reducing the amortization error introduced by the use of a conditional network
Non-parametric methods
Importance Weighted Autoencoder (IWAE) >> multiple samples >> computationally difficult


Variational Inference is widely used for solving a Bayesian inference problem. It is different from Markov Chain Monte Carlo(MCMC) methods, which rely on the Markov chain to reach equilibrium; in VI one can easily draw i.i.d. samples from the variational distribution, and enjoy lower variance in inference. However, vanilla VI has two major problems: overconfidence in prediction distribution and bad local optima with the unimodal posterior distribution. Paper \cite{main_Huang2018ImprovingEI} claims that the optimization process could limit the density of posterior distribution. The authors of this work aim to solve these issues with different objective functions and some optimization tricks. We also investigate closely related posterior collapse problem, where the generative model learns to ignore a subset of the latent variable. The paper \cite{lucas2019understanding} gives a general introduction to this phenomenon. One of the solutions to this problem is to use annealing strategies for inference, for example, alpha or beta annealing. 

The paper \cite{main_Huang2018ImprovingEI} states that due to the zero-forcing property of the KL the true posterior tends to be unimodal in usual variational inference, {the drawbacks of biasing}.
The author introduces the hybrid method of alpha annealing and Annealed importance sampling, called Annealed Variational Objectives (AVO). The method uses a highly flexible parametric form of the posterior distribution (assuming we have a rich family of approximate posterior at the hands). % , where learning of the auxiliary distribution, i.e. variational distribution, is through maximizing the ELBO.
% without changing prior distribution to richer variational family (that could influences on the performance of the inference). 
% Overconfidence arises because VI doesn't propagate uncertanty well especially it is a problem for amortized VI. As a result VI ignores some of latent variables. Не уверен в этой фразе

% Variational Inference(VI) has played an important role in Bayesian model uncertainty calibration and in unsupervised representation learning.  But unfortunately, the quality of variational inference is influenced by a phenomenon known as posterior collapse, where the generative model learns to ignore a subset of the latent variable. 

% Posterior collapse in Variational Autoencoders (VAEs) arises when the variational distribution closely matches the uninformative prior for a subset of latent variables. 

% \subsection*{Goal}
% During this project we are going to implement method from \cite{main_Huang2018ImprovingEI} and repeat experiments of this paper. 

% Experiments:
% \begin{enumerate}
%     \item Biased noise model.
%     \item Toy energy fitting.
%     \item Quantitative analysis on robustness to beta annealing.
%     \item Amortized inference on MNIST and CelebA datasets.
% \end{enumerate}

% We also want to demonstrate posterior collapse on toy example and show how method from AVO \cite{main_Huang2018ImprovingEI} can mitigate this problem. Also we want to check if Variational Auto Encoder (VAE) can be improved by the AVO method on the MNIST dataset.

% \subsection*{General plan}
% \begin{enumerate}
%     \item Re-implement Variational-Auto Encoder (VAE) \cite{kingma2013autoencoding} with parameters from \cite{TW:2016}. %\url{https://github.com/jmtomczak/vae_householder_flow}
%     \item Implement Hierarchical Variational Model (HVM) based on \cite{ranganath2015hierarchical}.
%     \item Implement Importance Weighted Auto-Encoder (IWAE) based on \cite{burda2015importance}.
%     \item Implement Hierarchical Variational Inference (HVI) method  based on \cite{ranganath2015hierarchical}.
%     \item Implement Annealed Variational Objectives (AVO) method \cite{main_Huang2018ImprovingEI}.
%     \item Analyze a behavior AVO method on toy examples.
%     \item Conduct experiments with VAE and AVO on the MNIST and CelebA datasets.
% \end{enumerate}

\section{Experiments}

VAE experiment on MNIST dataset

Same decoder and encoder
2 hidden layers with dimension 300
40 - latent space size
LeakyReLU activation function
Batch normalization
Optimizer - Adam (lr - 1e-3), batch_size=64, epochs=25
HVI - 5 stochastic transition operators (hidden size - 40)
Beta annealing
Beta0 = 0.2
Gamma  = 2e-4




\section{Results}

\section{Conclusion}

% We find that despite the representational capacity of the chosen family of approximate distributions in
% VI, the density that can be represented is still limited by the optimization process. We resolve this by
% incorporating annealed objectives into the training of hierarchical variational methods. Experimentally,
% we demonstrate (1) our method’s robustness to deterministic warm-up, (2) the benefits of encouraging
% exploration and (3) the downside of biasing the true posterior to be unimodal. Our method is
% orthogonal to finding a more rich family of variational distributions, and sheds light on an important
% optimization issue that has thus far been neglected in the amortized VI literature.

\section{Resources}

Github repository:
\url{https://github.com/alexey-pronkin/annealed}


\subsection*{Acknowledgements}

The project represents the paper \cite{main_Huang2018ImprovingEI}.
We use \cite{lucas2019understanding} as an introduction to the problem and \cite{Knoblauch2019GeneralizedVI} as an introduction to generalized variational inference problem. 

We planned to use and rewrite some code from \url{https://github.com/joelouismarino/iterative_inference/}, \url{https://github.com/jmtomczak/vae_householder_flow}, \url{https://github.com/AntixK/PyTorch-VAE}, , \url{https://github.com/haofuml/cyclical_annealing} and \url{https://github.com/ajayjain/lmconv}. (We assume, that first two repositories were used in the original paper \cite{main_Huang2018ImprovingEI} closed source code) 

We want to try to apply annealing strategies for some of SoTA AE for MNIST \url{https://paperswithcode.com/sota/image-generation-on-mnist} if we will have time.

\bibliographystyle{unsrt}
\bibliography{main}

\end{document}
